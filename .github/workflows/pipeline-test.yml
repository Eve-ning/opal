# This workflow will build opal, install its build, then runs the tests with pytest.


name: Test Model Pipeline Inference

on: [ pull_request ]
jobs:
  pipeline:
    timeout-minutes: 10
    runs-on: ubuntu-latest
    defaults:
      run:
        shell: bash
    steps:
      - name: Checkout repository with submodules
        uses: actions/checkout@v3
        with:
          submodules: true

      - name: Execute Pipeline
        working-directory: src/
        shell: bash
        run: |
          chmod +x pipeline.sh
          ./pipeline.sh cache

      - name: Check Dataset exists and is not empty
        if: always()
        working-directory: src/datasets/
        run: |
          dataset=*.csv
          [ -f "$dataset" ] || { echo "Dataset file not found"; exit 1; }
          [ -s "$dataset" ] || { echo "Dataset file is empty"; exit 1; }

      - name: Check Pipeline Cache exists and is not empty
        if: always()
        working-directory: src/.pipeline_cache/
        run: |
          pipeline_cache=cache.csv
          [ -f "$pipeline_cache" ] || { echo "Pipeline Cache file not found"; exit 1; }
          [ -s "$pipeline_cache" ] || { echo "Pipeline Cache file is empty"; exit 1; }

      - name: Check Dist exists and is not empty
        if: always()
        working-directory: src/dist/
        run: |
          dist=*.tar.gz2
          [ -f "$dist" ] || { echo "Dist file not found"; exit 1; }
          [ -s "$dist" ] || { echo "Dist file is empty"; exit 1; }

      - name: Preprocess Docker Logs
        working-directory: src/
        if: always()
        run: |
          for SVC in 1.preprocess 2.svness 3.preprocess 4.export; do
            docker compose \
            --profile files \
            -f preprocess/docker-compose.yml \
            --env-file preprocess/osu-data-docker/.env \
            logs $SVC > ${SVC}.log
          done

      - name: Export src as Artifact
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: src
          path: src/
